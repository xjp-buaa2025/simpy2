"""
å·¥è‰ºæµç¨‹æ¥å£
æä¾›å·¥è‰ºæµç¨‹çš„åˆ›å»ºã€è§£æã€éªŒè¯å’Œç®¡ç†åŠŸèƒ½

APIç«¯ç‚¹:
- GET /api/process/template: ä¸‹è½½CSVæ¨¡æ¿
- POST /api/process/parse-csv: è§£æä¸Šä¼ çš„CSV
- POST /api/process/validate: éªŒè¯å·¥è‰ºæµç¨‹DAG
- POST /api/process/save: ä¿å­˜å·¥è‰ºæµç¨‹
- GET /api/process/example: è·å–ç¤ºä¾‹å·¥è‰ºæµç¨‹
"""

import os
import io
import csv
from typing import List, Dict, Any, Optional, Set
from enum import Enum
from fastapi import APIRouter, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

router = APIRouter()


# ============ æšä¸¾å®šä¹‰ ============

class OpType(str, Enum):
    """æ“ä½œç±»å‹æšä¸¾"""
    H = "H"  # å–/æ”¾ (Handling)
    A = "A"  # è£…é… (Assembly)
    M = "M"  # æµ‹é‡ (Measurement) - å¯è§¦å‘è¿”å·¥
    T = "T"  # å·¥å…·æ“ä½œ (Tooling)
    D = "D"  # æ•°æ®è®°å½• (Data Recording)


# æ“ä½œç±»å‹å…ƒæ•°æ®
OP_TYPE_META = {
    OpType.H: {"zh": "å–/æ”¾", "en": "Handling", "color": "#3B82F6", "icon": "ğŸ“¦"},
    OpType.A: {"zh": "è£…é…", "en": "Assembly", "color": "#10B981", "icon": "ğŸ”§"},
    OpType.M: {"zh": "æµ‹é‡", "en": "Measurement", "color": "#F59E0B", "icon": "ğŸ“"},
    OpType.T: {"zh": "å·¥å…·æ“ä½œ", "en": "Tooling", "color": "#8B5CF6", "icon": "ğŸ› ï¸"},
    OpType.D: {"zh": "æ•°æ®è®°å½•", "en": "Data Recording", "color": "#6B7280", "icon": "ğŸ“"},
}


# ============ æ•°æ®æ¨¡å‹ ============

class ProcessNode(BaseModel):
    """å·¥è‰ºèŠ‚ç‚¹æ¨¡å‹"""
    step_id: str = Field(description="å”¯ä¸€æ­¥éª¤ID")
    task_name: str = Field(description="ä»»åŠ¡åç§°")
    op_type: OpType = Field(description="æ“ä½œç±»å‹ï¼ˆH/A/M/T/Dï¼‰")
    predecessors: str = Field(default="", description="å‰ç½®ä¾èµ–ï¼ˆåˆ†å·åˆ†éš”ï¼‰")
    std_duration: float = Field(ge=0, description="æ ‡å‡†å·¥æ—¶ï¼ˆåˆ†é’Ÿï¼‰")
    time_variance: float = Field(default=0.0, ge=0, description="æ—¶é—´æ³¢åŠ¨æ–¹å·®")
    work_load_score: int = Field(default=5, ge=1, le=10, description="REBAè´Ÿè·è¯„åˆ†")
    rework_prob: float = Field(default=0.0, ge=0, le=1, description="è¿”å·¥æ¦‚ç‡ï¼ˆä»…Mç±»æœ‰æ•ˆï¼‰")
    required_workers: int = Field(default=1, ge=1, description="æ‰€éœ€å·¥äººæ•°")
    required_tools: List[str] = Field(default=[], description="æ‰€éœ€å·¥å…·/è®¾å¤‡åˆ—è¡¨")
    station: str = Field(default="ST01", description="å·¥ä½ID")
    
    # å‰ç«¯åæ ‡ï¼ˆç”¨äºæµç¨‹å›¾ç¼–è¾‘å™¨ï¼‰
    x: float = Field(default=0, description="èŠ‚ç‚¹Xåæ ‡")
    y: float = Field(default=0, description="èŠ‚ç‚¹Yåæ ‡")
    
    def get_predecessor_list(self) -> List[str]:
        """è§£æå‰ç½®ä¾èµ–ä¸ºåˆ—è¡¨"""
        if not self.predecessors:
            return []
        return [p.strip() for p in self.predecessors.split(";") if p.strip()]
    
    def get_critical_equipment(self, critical_set: Set[str]) -> List[str]:
        """è·å–å…³é”®è®¾å¤‡ï¼ˆéœ€è¦æ’é˜Ÿçš„ï¼‰"""
        return [t for t in self.required_tools if t in critical_set]
    
    def get_common_tools(self, critical_set: Set[str]) -> List[str]:
        """è·å–æ™®é€šå·¥å…·ï¼ˆæ— é™ä¾›åº”ï¼‰"""
        return [t for t in self.required_tools if t not in critical_set]


class ProcessDefinition(BaseModel):
    """å·¥è‰ºæµç¨‹å®šä¹‰"""
    name: str = Field(default="æœªå‘½åæµç¨‹", description="æµç¨‹åç§°")
    description: str = Field(default="", description="æµç¨‹æè¿°")
    nodes: List[ProcessNode] = Field(default=[], description="å·¥è‰ºèŠ‚ç‚¹åˆ—è¡¨")
    
    def get_node_map(self) -> Dict[str, ProcessNode]:
        """è·å–èŠ‚ç‚¹æ˜ å°„å­—å…¸"""
        return {node.step_id: node for node in self.nodes}


class APIResponse(BaseModel):
    """ç»Ÿä¸€APIå“åº”æ ¼å¼"""
    success: bool
    message: str
    data: Optional[Any] = None


class ValidationResult(BaseModel):
    """éªŒè¯ç»“æœ"""
    valid: bool
    errors: List[str] = []
    warnings: List[str] = []
    node_count: int = 0
    edge_count: int = 0
    parallel_groups: int = 0


# ============ CSVæ¨¡æ¿ ============

CSV_TEMPLATE_HEADERS = [
    "step_id",
    "task_name", 
    "op_type",
    "predecessors",
    "std_duration",
    "time_variance",
    "work_load_score",
    "rework_prob",
    "required_workers",
    "required_tools",
    "station"
]

CSV_TEMPLATE_EXAMPLE = [
    ["S001", "å–å‹æ°”æœºè½¬å­", "H", "", "5", "1", "4", "0", "2", "åŠè£…è®¾å¤‡", "ST01"],
    ["S002", "å®‰è£…å‰æ£€æŸ¥", "M", "S001", "10", "2", "3", "0.05", "1", "æ£€æµ‹å°", "ST02"],
    ["S003", "è£…é…å‰è½´æ‰¿", "A", "S002", "15", "3", "6", "0", "2", "è£…é…å°", "ST03"],
    ["S004", "è£…é…åè½´æ‰¿", "A", "S002", "15", "3", "6", "0", "2", "è£…é…å°", "ST03"],
    ["S005", "å®‰è£…å¯†å°ä»¶", "A", "S003;S004", "8", "1.5", "5", "0", "1", "", "ST04"],
    ["S006", "åŠ¨å¹³è¡¡æµ‹è¯•", "M", "S005", "30", "5", "4", "0.1", "1", "åŠ¨å¹³è¡¡æœº", "ST05"],
    ["S007", "è®°å½•æµ‹è¯•æ•°æ®", "D", "S006", "5", "0.5", "2", "0", "1", "", "ST06"],
    ["S008", "æœ€ç»ˆè£…é…", "A", "S007", "20", "4", "7", "0", "2", "è£…é…å°", "ST07"],
    ["S009", "è¯•è½¦å‡†å¤‡", "T", "S008", "10", "2", "5", "0", "2", "è¯•è½¦å°", "ST08"],
    ["S010", "æ•´æœºè¯•è½¦", "M", "S009", "60", "10", "6", "0.15", "2", "è¯•è½¦å°", "ST08"],
]


# ============ APIç«¯ç‚¹ ============

@router.get("/template")
async def download_template():
    """
    ä¸‹è½½CSVæ¨¡æ¿
    
    è¿”å›å·¥è‰ºæµç¨‹CSVæ¨¡æ¿æ–‡ä»¶ï¼ŒåŒ…å«è¡¨å¤´å’Œç¤ºä¾‹æ•°æ®
    """
    output = io.StringIO()
    writer = csv.writer(output)
    
    # å†™å…¥è¡¨å¤´
    writer.writerow(CSV_TEMPLATE_HEADERS)
    
    # å†™å…¥ç¤ºä¾‹æ•°æ®
    for row in CSV_TEMPLATE_EXAMPLE:
        writer.writerow(row)
    
    output.seek(0)
    
    return StreamingResponse(
        io.BytesIO(output.getvalue().encode('utf-8-sig')),  # ä½¿ç”¨BOMä¾¿äºExcelè¯†åˆ«ä¸­æ–‡
        media_type="text/csv",
        headers={
            "Content-Disposition": "attachment; filename=process_template.csv"
        }
    )


@router.post("/parse-csv", response_model=APIResponse)
async def parse_csv(file: UploadFile = File(...)):
    """
    è§£æä¸Šä¼ çš„CSVæ–‡ä»¶
    
    å°†CSVæ–‡ä»¶è§£æä¸ºå·¥è‰ºæµç¨‹å®šä¹‰å¯¹è±¡
    """
    if not file.filename.endswith('.csv'):
        return APIResponse(
            success=False,
            message="è¯·ä¸Šä¼ CSVæ ¼å¼æ–‡ä»¶"
        )
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # å°è¯•ä¸åŒç¼–ç 
        for encoding in ['utf-8-sig', 'utf-8', 'gbk', 'gb2312']:
            try:
                text = content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        else:
            return APIResponse(
                success=False,
                message="æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ï¼Œè¯·ä½¿ç”¨UTF-8ç¼–ç "
            )
        
        # è§£æCSV
        reader = csv.DictReader(io.StringIO(text))
        nodes = []
        errors = []
        
        for row_num, row in enumerate(reader, start=2):
            try:
                # è§£ærequired_tools
                tools_str = row.get('required_tools', '').strip()
                tools = [t.strip() for t in tools_str.split(';') if t.strip()] if tools_str else []
                
                node = ProcessNode(
                    step_id=row.get('step_id', '').strip(),
                    task_name=row.get('task_name', '').strip(),
                    op_type=OpType(row.get('op_type', 'A').strip().upper()),
                    predecessors=row.get('predecessors', '').strip(),
                    std_duration=float(row.get('std_duration', 0)),
                    time_variance=float(row.get('time_variance', 0)),
                    work_load_score=int(row.get('work_load_score', 5)),
                    rework_prob=float(row.get('rework_prob', 0)),
                    required_workers=int(row.get('required_workers', 1)),
                    required_tools=tools,
                    station=row.get('station', 'ST01').strip() or 'ST01'
                )
                nodes.append(node)
            except Exception as e:
                errors.append(f"ç¬¬{row_num}è¡Œè§£æé”™è¯¯: {str(e)}")
        
        if errors:
            return APIResponse(
                success=False,
                message=f"CSVè§£æå­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯",
                data={"errors": errors, "parsed_count": len(nodes)}
            )
        
        process = ProcessDefinition(
            name=file.filename.replace('.csv', ''),
            nodes=nodes
        )
        
        return APIResponse(
            success=True,
            message=f"æˆåŠŸè§£æ {len(nodes)} ä¸ªå·¥è‰ºèŠ‚ç‚¹",
            data=process.model_dump()
        )
        
    except Exception as e:
        return APIResponse(
            success=False,
            message=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        )


@router.post("/validate", response_model=APIResponse)
async def validate_process(process: ProcessDefinition):
    """
    éªŒè¯å·¥è‰ºæµç¨‹DAG
    
    æ£€æŸ¥å·¥è‰ºæµç¨‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ï¼š
    - èŠ‚ç‚¹IDå”¯ä¸€æ€§
    - ä¾èµ–å…³ç³»æœ‰æ•ˆæ€§
    - æ— å¾ªç¯ä¾èµ–
    - æœ‰èµ·å§‹å’Œç»“æŸèŠ‚ç‚¹
    """
    errors = []
    warnings = []
    
    # æ„å»ºèŠ‚ç‚¹æ˜ å°„
    node_map = process.get_node_map()
    node_ids = set(node_map.keys())
    
    # æ£€æŸ¥èŠ‚ç‚¹IDå”¯ä¸€æ€§
    if len(node_ids) != len(process.nodes):
        errors.append("å­˜åœ¨é‡å¤çš„èŠ‚ç‚¹ID")
    
    # æ£€æŸ¥ä¾èµ–å…³ç³»
    edge_count = 0
    for node in process.nodes:
        predecessors = node.get_predecessor_list()
        for pred_id in predecessors:
            if pred_id not in node_ids:
                errors.append(f"èŠ‚ç‚¹ '{node.step_id}' çš„å‰ç½®ä¾èµ– '{pred_id}' ä¸å­˜åœ¨")
            else:
                edge_count += 1
    
    # æ£€æŸ¥æ˜¯å¦æœ‰èµ·å§‹èŠ‚ç‚¹ï¼ˆæ— å‰ç½®ä¾èµ–çš„èŠ‚ç‚¹ï¼‰
    start_nodes = [n for n in process.nodes if not n.get_predecessor_list()]
    if not start_nodes:
        errors.append("æ²¡æœ‰æ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰å‰ç½®ä¾èµ–ï¼‰")
    
    # æ£€æŸ¥å¾ªç¯ä¾èµ–ï¼ˆç®€å•çš„DFSæ£€æµ‹ï¼‰
    def has_cycle() -> bool:
        visited = set()
        rec_stack = set()
        
        def dfs(node_id: str) -> bool:
            visited.add(node_id)
            rec_stack.add(node_id)
            
            # è·å–åç»§èŠ‚ç‚¹
            for n in process.nodes:
                if node_id in n.get_predecessor_list():
                    if n.step_id not in visited:
                        if dfs(n.step_id):
                            return True
                    elif n.step_id in rec_stack:
                        return True
            
            rec_stack.remove(node_id)
            return False
        
        for node in process.nodes:
            if node.step_id not in visited:
                if dfs(node.step_id):
                    return True
        return False
    
    if has_cycle():
        errors.append("æµç¨‹å›¾å­˜åœ¨å¾ªç¯ä¾èµ–")
    
    # æ£€æŸ¥Mç±»å‹èŠ‚ç‚¹çš„è¿”å·¥æ¦‚ç‡
    for node in process.nodes:
        if node.op_type == OpType.M and node.rework_prob > 0:
            if node.rework_prob > 0.5:
                warnings.append(f"èŠ‚ç‚¹ '{node.step_id}' è¿”å·¥æ¦‚ç‡ {node.rework_prob} è¾ƒé«˜")
        elif node.op_type != OpType.M and node.rework_prob > 0:
            warnings.append(f"èŠ‚ç‚¹ '{node.step_id}' éæµ‹é‡ç±»å‹ä½†è®¾ç½®äº†è¿”å·¥æ¦‚ç‡")
    
    # æ£€æŸ¥å·¥ä½œè´Ÿè·
    high_load_nodes = [n for n in process.nodes if n.work_load_score >= 8]
    if len(high_load_nodes) > len(process.nodes) * 0.3:
        warnings.append(f"æœ‰ {len(high_load_nodes)} ä¸ªé«˜è´Ÿè·èŠ‚ç‚¹ï¼ˆâ‰¥8åˆ†ï¼‰ï¼Œå¯èƒ½å½±å“å·¥äººæ•ˆç‡")
    
    # è®¡ç®—å¹¶è¡Œç»„æ•°ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
    parallel_groups = len(start_nodes)
    
    result = ValidationResult(
        valid=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        node_count=len(process.nodes),
        edge_count=edge_count,
        parallel_groups=parallel_groups
    )
    
    return APIResponse(
        success=result.valid,
        message="éªŒè¯é€šè¿‡" if result.valid else f"éªŒè¯å¤±è´¥ï¼Œå­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯",
        data=result.model_dump()
    )


@router.post("/save", response_model=APIResponse)
async def save_process(process: ProcessDefinition):
    """
    ä¿å­˜å·¥è‰ºæµç¨‹
    
    å°†å·¥è‰ºæµç¨‹ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
    """
    try:
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{process.name.replace(' ', '_')}.json"
        data_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
            "data"
        )
        os.makedirs(data_dir, exist_ok=True)
        
        filepath = os.path.join(data_dir, filename)
        
        # ä¿å­˜JSON
        import json
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(process.model_dump(), f, ensure_ascii=False, indent=2)
        
        return APIResponse(
            success=True,
            message=f"æµç¨‹å·²ä¿å­˜åˆ° {filename}",
            data={"filepath": filepath}
        )
    except Exception as e:
        return APIResponse(
            success=False,
            message=f"ä¿å­˜å¤±è´¥: {str(e)}"
        )


@router.get("/example", response_model=APIResponse)
async def get_example_process():
    """
    è·å–ç¤ºä¾‹å·¥è‰ºæµç¨‹
    
    è¿”å›ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹å·¥è‰ºæµç¨‹å®šä¹‰
    """
    example_nodes = []
    for i, row in enumerate(CSV_TEMPLATE_EXAMPLE):
        tools = [t.strip() for t in row[9].split(';') if t.strip()] if row[9] else []
        node = ProcessNode(
            step_id=row[0],
            task_name=row[1],
            op_type=OpType(row[2]),
            predecessors=row[3],
            std_duration=float(row[4]),
            time_variance=float(row[5]),
            work_load_score=int(row[6]),
            rework_prob=float(row[7]),
            required_workers=int(row[8]),
            required_tools=tools,
            x=100 + (i % 3) * 200,
            y=100 + (i // 3) * 120
        )
        example_nodes.append(node)
    
    process = ProcessDefinition(
        name="èˆªç©ºå‘åŠ¨æœºè£…é…ç¤ºä¾‹æµç¨‹",
        description="åŒ…å«å‹æ°”æœºè½¬å­è£…é…ã€åŠ¨å¹³è¡¡æµ‹è¯•å’Œæ•´æœºè¯•è½¦çš„æ ‡å‡†æµç¨‹",
        nodes=example_nodes
    )
    
    return APIResponse(
        success=True,
        message="è·å–ç¤ºä¾‹æµç¨‹æˆåŠŸ",
        data=process.model_dump()
    )


# å¤æ‚ç¤ºä¾‹æ•°æ® - èˆªç©ºå‘åŠ¨æœºå®Œæ•´è£…é…æµç¨‹ï¼ˆçº¦35ä¸ªèŠ‚ç‚¹ï¼Œæœˆäº§3å°ï¼‰
COMPLEX_EXAMPLE_NODES = [
    # é˜¶æ®µ1: å‡†å¤‡ä¸æ¥æ–™æ£€éªŒ
    ("S001", "é£æ‰‡å¶ç‰‡æ£€éªŒ", "M", "", 25, 3, 4, 0.03, 1, "æ£€æµ‹å°"),
    ("S002", "å‹æ°”æœºå¶ç‰‡æ£€éªŒ", "M", "", 30, 4, 4, 0.04, 1, "æ£€æµ‹å°"),
    ("S003", "å·¥è£…å‡†å¤‡", "H", "", 20, 2, 4, 0, 2, "è£…é…å°"),
    # é˜¶æ®µ2: ä½å‹å‹æ°”æœºè£…é…
    ("S101", "ä½å‹è½¬å­åŠè£…", "H", "S001;S003", 25, 4, 8, 0, 3, "åŠè½¦"),
    ("S102", "ä½å‹å¶ç‰‡å®‰è£…", "A", "S101", 90, 10, 7, 0, 2, "è£…é…å°"),
    ("S103", "ä½å‹å‹æ°”æœºæµ‹é‡", "M", "S102", 30, 4, 5, 0.08, 2, "æ£€æµ‹å°"),
    ("S104", "ä½å‹åŠ¨å¹³è¡¡", "M", "S103", 60, 8, 6, 0.10, 2, "åŠ¨å¹³è¡¡æœº"),
    # é˜¶æ®µ3: é«˜å‹å‹æ°”æœºè£…é…
    ("S201", "é«˜å‹è½¬å­åŠè£…", "H", "S002;S104", 25, 4, 8, 0, 3, "åŠè½¦"),
    ("S202", "é«˜å‹å¶ç‰‡å®‰è£…", "A", "S201", 150, 15, 7, 0, 2, "è£…é…å°"),
    ("S203", "é«˜å‹å‹æ°”æœºæµ‹é‡", "M", "S202", 35, 5, 5, 0.08, 2, "æ£€æµ‹å°"),
    ("S204", "é«˜å‹åŠ¨å¹³è¡¡", "M", "S203", 70, 10, 6, 0.12, 2, "åŠ¨å¹³è¡¡æœº"),
    # é˜¶æ®µ4: ç‡ƒçƒ§å®¤è£…é…
    ("S301", "ç‡ƒçƒ§å®¤è£…é…", "A", "S204", 120, 12, 7, 0, 2, "è£…é…å°"),
    ("S302", "ç‡ƒçƒ§å®¤å¯†å°æ£€æŸ¥", "M", "S301", 40, 5, 5, 0.06, 2, "æ£€æµ‹å°"),
    # é˜¶æ®µ5: æ¶¡è½®è£…é…
    ("S401", "æ¶¡è½®è£…é…", "A", "S302", 180, 18, 8, 0, 2, "è£…é…å°"),
    ("S402", "æ¶¡è½®é—´éš™æµ‹é‡", "M", "S401", 35, 5, 5, 0.08, 2, "æ£€æµ‹å°"),
    ("S403", "æ¶¡è½®åŠ¨å¹³è¡¡", "M", "S402", 75, 10, 6, 0.10, 2, "åŠ¨å¹³è¡¡æœº"),
    # é˜¶æ®µ6: è½´ç³»è£…é…
    ("S501", "è½´ç³»è£…é…", "A", "S403", 100, 10, 7, 0, 2, "è£…é…å°"),
    ("S502", "è½´ç³»æµ‹é‡", "M", "S501", 45, 6, 5, 0.06, 2, "æ£€æµ‹å°"),
    # é˜¶æ®µ7: é™„ä»¶æœºåŒ£è£…é…
    ("S601", "é™„ä»¶æœºåŒ£è£…é…", "A", "S502", 120, 12, 6, 0, 2, "è£…é…å°"),
    ("S602", "é™„ä»¶ç³»ç»Ÿæ£€æŸ¥", "M", "S601", 30, 4, 5, 0.05, 2, "æ£€æµ‹å°"),
    # é˜¶æ®µ8: æ€»è£…
    ("S701", "æ€»è£…åŠè£…", "H", "S602", 35, 5, 9, 0, 4, "åŠè½¦"),
    ("S702", "ç®¡è·¯è¿æ¥", "A", "S701", 150, 15, 6, 0, 2, ""),
    ("S703", "ç”µæ°”å®‰è£…", "A", "S702", 80, 8, 5, 0, 2, ""),
    # é˜¶æ®µ9: æ€»è£…æ£€æµ‹
    ("S801", "æ°”å¯†æ€§æµ‹è¯•", "M", "S703", 60, 8, 5, 0.05, 2, "æ£€æµ‹å°"),
    ("S802", "ç”µæ°”æµ‹è¯•", "M", "S801", 45, 6, 5, 0.04, 2, "æ£€æµ‹å°"),
    # é˜¶æ®µ10: è¯•è½¦
    ("S901", "è¯•è½¦å‡†å¤‡", "H", "S802", 40, 5, 6, 0, 2, "è¯•è½¦å°"),
    ("S902", "å®‰è£…è¯•è½¦å°", "H", "S901", 50, 6, 8, 0, 4, "è¯•è½¦å°;åŠè½¦"),
    ("S903", "æ…¢è½¦è¯•è½¦", "M", "S902", 90, 12, 6, 0.08, 3, "è¯•è½¦å°"),
    ("S904", "é«˜é€Ÿè¯•è½¦", "M", "S903", 120, 15, 7, 0.10, 3, "è¯•è½¦å°"),
    ("S905", "æ•°æ®åˆ†æ", "D", "S904", 60, 8, 4, 0, 2, ""),
    ("S906", "è¯•è½¦åæ£€æŸ¥", "M", "S905", 45, 6, 5, 0.05, 2, "æ£€æµ‹å°"),
    # é˜¶æ®µ11: æœ€ç»ˆå¤„ç†
    ("S1001", "ä¸‹å°æ¸…æ´", "H", "S906", 50, 5, 6, 0, 2, "åŠè½¦"),
    ("S1002", "æ–‡ä»¶æ•´ç†", "D", "S1001", 40, 5, 3, 0, 1, ""),
    ("S1003", "è´¨é‡å®¡æ ¸", "D", "S1002", 30, 4, 3, 0, 2, ""),
    ("S1004", "åŒ…è£…å…¥åº“", "H", "S1003", 35, 4, 6, 0, 2, ""),
]


@router.get("/example-complex", response_model=APIResponse)
async def get_complex_example_process():
    """
    è·å–å¤æ‚ç¤ºä¾‹å·¥è‰ºæµç¨‹
    
    è¿”å›èˆªç©ºå‘åŠ¨æœºå®Œæ•´è£…é…æµç¨‹ï¼ˆçº¦35ä¸ªèŠ‚ç‚¹ï¼‰
    è®¾è®¡ä¸ºåœ¨äººå› ä¸èµ„æºåŒé‡çº¦æŸä¸‹æœˆäº§çº¦3å°å‘åŠ¨æœº
    
    æ€»æ ‡å‡†å·¥æ—¶çº¦2000åˆ†é’Ÿï¼ˆçº¦33å°æ—¶ï¼‰ï¼Œè€ƒè™‘ï¼š
    - 6åå·¥äººï¼Œéƒ¨åˆ†ä»»åŠ¡éœ€è¦2-4äºº
    - å…³é”®è®¾å¤‡çº¦æŸï¼ˆåŠ¨å¹³è¡¡æœº2å°ã€è¯•è½¦å°1å°ç­‰ï¼‰
    - ä¼‘æ¯è§„åˆ™ï¼ˆè¿ç»­å·¥ä½œ50åˆ†é’Ÿä¼‘æ¯ã€é«˜è´Ÿè·ä»»åŠ¡åä¼‘æ¯ï¼‰
    - è¿”å·¥æ¦‚ç‡ï¼ˆMç±»å‹ä»»åŠ¡5%-12%è¿”å·¥ç‡ï¼‰
    """
    nodes = []
    for i, row in enumerate(COMPLEX_EXAMPLE_NODES):
        tools = [t.strip() for t in row[9].split(';') if t.strip()] if row[9] else []
        node = ProcessNode(
            step_id=row[0],
            task_name=row[1],
            op_type=OpType(row[2]),
            predecessors=row[3],
            std_duration=float(row[4]),
            time_variance=float(row[5]),
            work_load_score=int(row[6]),
            rework_prob=float(row[7]),
            required_workers=int(row[8]),
            required_tools=tools,
            x=100 + (i % 4) * 180,
            y=80 + (i // 4) * 90
        )
        nodes.append(node)
    
    process = ProcessDefinition(
        name="èˆªç©ºå‘åŠ¨æœºå®Œæ•´è£…é…æµç¨‹",
        description="å®Œæ•´çš„èˆªç©ºå‘åŠ¨æœºè£…é…æµç¨‹ï¼ŒåŒ…å«11ä¸ªé˜¶æ®µ35ä¸ªå·¥åºï¼Œè®¾è®¡æœˆäº§é‡çº¦3å°",
        nodes=nodes
    )
    
    return APIResponse(
        success=True,
        message="è·å–å¤æ‚ç¤ºä¾‹æµç¨‹æˆåŠŸï¼ˆ35ä¸ªèŠ‚ç‚¹ï¼Œé¢„è®¡æœˆäº§3å°ï¼‰",
        data=process.model_dump()
    )


@router.get("/op-types", response_model=APIResponse)
async def get_op_types():
    """
    è·å–æ“ä½œç±»å‹å…ƒæ•°æ®
    
    è¿”å›æ‰€æœ‰æ“ä½œç±»å‹çš„ä¸­è‹±æ–‡åç§°ã€é¢œè‰²å’Œå›¾æ ‡
    """
    data = []
    for op_type in OpType:
        meta = OP_TYPE_META[op_type]
        data.append({
            "value": op_type.value,
            "zh": meta["zh"],
            "en": meta["en"],
            "color": meta["color"],
            "icon": meta["icon"]
        })
    
    return APIResponse(
        success=True,
        message="è·å–æ“ä½œç±»å‹æˆåŠŸ",
        data=data
    )


@router.post("/export-csv", response_model=None)
async def export_process_csv(process: ProcessDefinition):
    """
    å¯¼å‡ºå·¥è‰ºæµç¨‹ä¸ºCSV
    """
    output = io.StringIO()
    writer = csv.writer(output)
    
    # å†™å…¥è¡¨å¤´
    writer.writerow(CSV_TEMPLATE_HEADERS)
    
    # å†™å…¥æ•°æ®
    for node in process.nodes:
        writer.writerow([
            node.step_id,
            node.task_name,
            node.op_type.value,
            node.predecessors,
            node.std_duration,
            node.time_variance,
            node.work_load_score,
            node.rework_prob,
            node.required_workers,
            ";".join(node.required_tools)
        ])
    
    output.seek(0)
    filename = f"{process.name.replace(' ', '_')}.csv"
    
    return StreamingResponse(
        io.BytesIO(output.getvalue().encode('utf-8-sig')),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={filename}"
        }
    )
